{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting up hadoop cluster\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Setting up environment:\n",
    "\n",
    "1. Oracle Box: Import Appliance, assign 1 CPU and 2GB memory. Save the imported VMs to a specific address on your drive by setting it up in base.\n",
    "2. Don't forget to \"reinitialize the MAC address for all network cards.\" (In Virtual Box version 6 and higher it is \"Generate new MAC addresses for all network adapters.\")\n",
    "3. After importing them and renaming their group, change their network setting to host only.\n",
    "4. These three VMs have two users: \n",
    "root with pass root\n",
    "hadoop with pass hadoop\n",
    "5. Check you host (in my case windows) IP by going to control panel>network and sharing center > Change adapter settign > Check ethernet 5 > properties > click on IPV4 and see your IP. In my case 192.168.56.1.\n",
    "6. Back in master node (CentOS vm) open terminal from Applications? Favorites> Terminal\n",
    "7. In terminal type nmtui to set the IP: \n",
    "disable IPV6;\n",
    "make IPV4 manual and set it in the same IP range as your host machine;\n",
    "set a host name: in this case node-master\n",
    "8. in terminal type ifconfig to see if IP is set properly.\n",
    "9. type exit to exit terminal.\n",
    "10. In wired settings disable DNS and route.\n",
    "11. From node-master ping host machine: ping 192.168.56.1\n",
    "12. From host machine ping node-master: ping 192.168.56.10\n",
    "13. Repeat steps 3 to 12 for all three nodes.\n",
    "14. Open ssh terminal by mobaxterm to each of your machines. SSH port is 22.\n",
    "15. To allow each node ping the other one with alias, we need to setup FQDN (Fully Qualified Domain Name). Consider that all nodes can see each other, but nodes do not see each others' names. To resolve this in windows: \n",
    "\n",
    "C:\\Windows\\System32\\drivers\\etc\\hosts\n",
    "\n",
    "Then edit hosts like below:\n",
    "\n",
    "```\n",
    "192.168.56.2 Node2\n",
    "192.168.1.1 PC1 (This is my own laptop.)\n",
    "192.168.56.10 node-master\n",
    "192.168.56.11 node1\n",
    "192.168.56.12 node2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is not necessary for our windows machine (host), but for our nodes we need to do it.\n",
    "\n",
    "On Linux nodes FQDN can set up on /etc/hosts\n",
    "You may need to su to root user:\n",
    "    ```\n",
    "+ su\n",
    "+ enter password for root\n",
    "+ cd .. to get to root\n",
    "+ cd etc\n",
    "+ vi hosts\n",
    "\n",
    "Hit insert button on key board to be able to add lines to hosts file. Add\n",
    "```\n",
    "192.168.56.10 node-master\n",
    "192.168.56.11 node1\n",
    "192.168.56.12 node2\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy this file to all the other nodes or repeat step 15 for all of them.\n",
    "\n",
    "To edit with vi, hit insert to be able to edit the file. To quit without saving use Esc, then type :q! To save and quit hit Esc, then type :wq (Write and Quit).\n",
    "\n",
    "16. You can ssh from one node to another. For example on ndoe-master you can:\n",
    "+ ssh node1\n",
    "Enter password and you are connected to node1.\n",
    "\n",
    "With user hadoop, you will be in /home/hadoop.\n",
    "\n",
    "17. To connect passwordless to your servers:\n",
    "I want user hadoop from node-master be able to connect to ndoe1 and node2 with ssh without password.\n",
    "To do so (passwordless ssh):\n",
    "+ open a session to node-master with the user you want to connect (in this case, hadoop).\n",
    "+ On node-master with user hadoop generate a key:\n",
    "+ ssh-keygen -b 4096\n",
    "4096 is number of bits of the key.\n",
    "The generated key by default is at /home/hadoop/.ssh/id_rsa. You can cat it.\n",
    "cat /home/hadoop/.ssh/id_rsa.pub\n",
    "\n",
    "18- Copy the generated key (be carfeul, just public key) to all other nodes. This includes master as well.\n",
    "+ ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub hadoop@node-master\n",
    "\n",
    "For the first time it will ask for password, but afterwards you can ssh passwordless.\n",
    "\n",
    "19- Repeat step 18 for node1 and node2:\n",
    "+ ssh-copy-id -i /home/hadoop/.ssh/id_rsa.pub hadoop@node1\n",
    "\n",
    "If you have n nodes, you have to do it for all of them.\n",
    "\n",
    "20- For nodes to be able to communicate with each other they need to see each others' IP and ports. At this stage we need to open firewalls.\n",
    "\n",
    "Add the ports to firewall:\n",
    "\n",
    "In hadoop 3 we need to open 9870, 9871, 9866, 9867 etc for different services of hadoop. We need to do it on each server for each port.\n",
    "\n",
    "+ sudo firewall-cmd —zone=public —add-port=9000/tcp —permanent\n",
    "+ sudo firewall-cmd reload\n",
    "\n",
    "The ports we need to open for the beginning are as follows:\n",
    "\n",
    "```\n",
    "namenode HTTP Port: 9870\n",
    "Namenode HTTPS Port: 9871\n",
    "Datanode HTTP Port: 9864\n",
    "Datanode HTTPS Port: 9865\n",
    "SecondaryNN HTTP Port: 9868\n",
    "SecoondaryNN HTTPS Port: 9869\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing replication factor and block size for individual files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to change file relication factor and block size while putting a file into hdfs.\n",
    "\n",
    "```\n",
    "hdfs dfs -Ddfs.blocksize=256000000 -put /home/hadoop/file.xml /xmlfile\n",
    "hdfs dfs -setrep 2 /data/retail/file.csv\n",
    "```\n",
    "\n",
    "In specific files and direcotries where data volume is low and it is necessary to be available on all servers, like spark config files, we need to increase data locality and put it on all nodes. We don' have to, but it is a good idea to increase replication factor.\n",
    "```\n",
    "hdfs dfs -setrep 95 /data/file.xml\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Changing user access"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| User | Group | Other|\n",
    "|----------|:-------------:|------:|\n",
    "| rwx  | rwx   | rwx |\n",
    "\n",
    "+ r: read\n",
    "+ w: write\n",
    "+ x: execute\n",
    "\n",
    "+ To enable security change hdfs-site.xml\n",
    "```\n",
    "dfs,namenode.acls.enabled\n",
    "hdfs getconf -confskey dfs.namenode.acls.enabled\n",
    "```\n",
    "\n",
    "Change dfs.namenode.acls.enabled to true in hdfs-site.xml. As usual copy this on all nodes.\n",
    "```\n",
    "scp /opt/hadoop/etc/hadoop/*.xml node1:/opt/hadoop/etc/hadoop/\n",
    "scp /opt/hadoop/etc/hadoop/*.xml node2:/opt/hadoop/etc/hadoop/\n",
    "\n",
    "```\n",
    "\n",
    "Stop and start hdfs:\n",
    "\n",
    "```\n",
    "stop-dfs.sh\n",
    "start-dfs.sh\n",
    "```\n",
    "\n",
    "+ I want to give user dr.who read write and execute access to directory data/crm.\n",
    "\n",
    "```\n",
    "hdfs dfs -setfacl --set user::---,user:hadoop:rwx,user:dr.who:rwx,group::r--,other::r-- /data/crm\n",
    "\n",
    "```\n",
    "\n",
    "+ -setfacl: set file access control list\n",
    "+ You can check your user with whoami.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hdfs-site.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
    "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
    "<!--\n",
    "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "  you may not use this file except in compliance with the License.\n",
    "  You may obtain a copy of the License at\n",
    "\n",
    "    http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "  Unless required by applicable law or agreed to in writing, software\n",
    "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "  See the License for the specific language governing permissions and\n",
    "  limitations under the License. See accompanying LICENSE file.\n",
    "-->\n",
    "\n",
    "<!-- Put site-specific property overrides in this file. -->\n",
    "\n",
    "<configuration>\n",
    "  \n",
    "  <property>\n",
    "    <name>dfs.replication</name>\n",
    "    <value>1</value>\n",
    "  </property>\n",
    "  \n",
    "  <property>\n",
    "    <name>dfs.datanode.data.dir</name>\n",
    "    <value>/opt/data</value>\n",
    "  </property>\n",
    "  \n",
    "    <property>\n",
    "    <name>dfs.namenode.name.dir</name>\n",
    "    <value>/opt/data</value>\n",
    "  </property>\n",
    "  \n",
    "  <property>\n",
    "    <name> dfs.namenode.acls.enabled </name>\n",
    "    <value> true </value>\n",
    "  </property> \n",
    "  \n",
    "\n",
    "</configuration>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nodes status\n",
    "\n",
    "Each node is available at http://node1:9864/datanode.html\n",
    "\n",
    "+ hadoop UI port: 9870\n",
    "+ data node's port: 9864"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating new user and groups\n",
    "\n",
    "+ The following users are created at os level:\n",
    "\n",
    "```\n",
    "sudo groupadd datasxience\n",
    "sudo useradd -m user1\n",
    "sudo passwd user1\n",
    "sudo usermod -aG datascience user1\n",
    "```\n",
    "\n",
    "passwd sets a password. Here I set password \"user1\" for user \"user1\".\n",
    "\n",
    "Now user1 is added to group datascience. \n",
    "\n",
    "+ This user must be created on both master (namenode) and all datanodes.\n",
    "+ To make sure user1 is created successfully switch user at os level:\n",
    "\n",
    "```\n",
    "su user1\n",
    "```\n",
    "\n",
    "+ granting access to use1\n",
    "\n",
    "```\n",
    "hdfs dfs -setfacl --set user::---,user:hadoop:rwx,user:user1:rwx,user:dr.who:r--,group::---,other::--- /datascience\n",
    "```\n",
    "\n",
    "Now that user1 has write access to directory /datascience we can switch user and write inside it:\n",
    "\n",
    "```\n",
    "su user1\n",
    "hdfs dfs -put  /opt/hadoop/testdata/file.txt /datascience\n",
    "```\n",
    "\n",
    "Make sureuser1 also has HJava and hadoop home set in .bashrc file in /home/user1.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1: How to change user in mobaxtem to see the GUI part?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## .bashrc file of user1\n",
    "\n",
    "```\n",
    "export JAVA_HOME=/opt/java\n",
    "PATH=$JAVA_HOME/bin:$PATH\n",
    "export PATH\n",
    "\n",
    "export HADOOP_HOME=/opt/hadoop\n",
    "PATH=$HADOOP_HOME/bin:$HADOOP_HOME/sbin:$PATH\n",
    "export PATH\n",
    "```\n",
    "\n",
    "+ Copy .bashrc from node-master to node1 and node2 (datanodes). \n",
    "+ source .bashrc file on all nodes to apply changes.\n",
    "\n",
    "```\n",
    "source .bashrc\n",
    "```\n",
    "\n",
    "**Note that if you set .bashrc in /etc/.bashrc you don't need to set it for each user.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "hdfs dfs -mkdir ds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storage policy or tierd storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Tape\n",
    "+ SSD\n",
    "+ SAN\n",
    "+ NAS\n",
    "\n",
    "Stoage policies:\n",
    "+ Hot\n",
    "+ Cold\n",
    "+ Warm\n",
    "+ All_SSD\n",
    "+ One_SSD\n",
    "+ Lazy_Persit\n",
    "\n",
    "Infrastructure team provides new directories and locations to us.\n",
    "\n",
    "+ For setting policies, make sure dfs.storage.policy.enabled is true. By default it is true. \n",
    "+ Copy the new hdfs-site.xml file to all data nodes.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "### hdfs-site.xml\n",
    "\n",
    "```\n",
    "<configuration>\n",
    "  \n",
    "  <property>\n",
    "    <name>dfs.replication</name>\n",
    "    <value>1</value>\n",
    "  </property>\n",
    "  \n",
    "  <property>\n",
    "    <name>dfs.datanode.data.dir</name>\n",
    "    <value>[DISK]file:///opt/data,[SSD]file:///opt/ssd_disk,[ARCHIVE]file:///opt/low_speed</value>\n",
    "  </property>\n",
    "  \n",
    "    <property>\n",
    "    <name>dfs.namenode.name.dir</name>\n",
    "    <value>/opt/data</value>\n",
    "  </property>\n",
    "  \n",
    "  <property>\n",
    "    <name> dfs.namenode.acls.enabled </name>\n",
    "    <value> true </value>\n",
    "  </property> \n",
    "  \n",
    "\n",
    "</configuration>\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After making changes to hdfs-site.xml and copy it to all datanodes, stop-dfs.sh and start-dfs.sh to apply the chnages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```\n",
    "hdfs storagepolicies -listPolicies\n",
    "```\n",
    "\n",
    "Output:\n",
    "    ```\n",
    "    lock Storage Policies:\n",
    "        BlockStoragePolicy{PROVIDED:1, storageTypes=[PROVIDED, DISK], creationFallbacks=[PROVIDED, DISK], replicationFallbacks=[PROVIDED, DISK]}\n",
    "        BlockStoragePolicy{COLD:2, storageTypes=[ARCHIVE], creationFallbacks=[], replicationFallbacks=[]}\n",
    "        BlockStoragePolicy{WARM:5, storageTypes=[DISK, ARCHIVE], creationFallbacks=[DISK, ARCHIVE], replicationFallbacks=[DISK, ARCHIVE]}\n",
    "        BlockStoragePolicy{HOT:7, storageTypes=[DISK], creationFallbacks=[], replicationFallbacks=[ARCHIVE]}\n",
    "        BlockStoragePolicy{ONE_SSD:10, storageTypes=[SSD, DISK], creationFallbacks=[SSD, DISK], replicationFallbacks=[SSD, DISK]}\n",
    "        BlockStoragePolicy{ALL_SSD:12, storageTypes=[SSD], creationFallbacks=[DISK], replicationFallbacks=[DISK]}\n",
    "        BlockStoragePolicy{LAZY_PERSIST:15, storageTypes=[RAM_DISK, DISK], creationFallbacks=[DISK], replicationFallbacks=[DISK]}\n",
    "\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
